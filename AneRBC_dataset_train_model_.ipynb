{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21931,
     "status": "ok",
     "timestamp": 1747059048591,
     "user": {
      "displayName": "Long Vu",
      "userId": "17415940645631717193"
     },
     "user_tz": 240
    },
    "id": "gDnGmBcQdHBo",
    "outputId": "44653920-0cf5-4b7e-829f-d8e33504d9ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2HZ5RKZi6yg"
   },
   "source": [
    "# Train the Hybrid CBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3700,
     "status": "ok",
     "timestamp": 1747059052293,
     "user": {
      "displayName": "Long Vu",
      "userId": "17415940645631717193"
     },
     "user_tz": 240
    },
    "id": "UdcrEYZRY2Ij"
   },
   "outputs": [],
   "source": [
    "# === dataset.py =============================================================\n",
    "\"\"\"PyTorch Dataset for the AneRBC hybrid Concept‑Bottleneck pipeline.\"\"\"\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Union, Dict, Optional\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class AneRBC(Dataset):\n",
    "    \"\"\"Single split of the AneRBC manifest.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_path : str or Path\n",
    "        Path to the manifest CSV.\n",
    "    split : str\n",
    "        'train', 'val', or 'test'.\n",
    "    concept_names : list[str] | None\n",
    "        Global ordered list of concept keys.  If `None`, will be inferred\n",
    "        from the *first* row of this split (legacy behaviour).\n",
    "    transform : callable | None\n",
    "        Vision transform.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_path: Union[str, Path],\n",
    "        split: str = \"train\",\n",
    "        transform=None,\n",
    "        *,\n",
    "        concept_names: Optional[List[str]] = None,\n",
    "        val_ratio: float = 0.15,\n",
    "        test_ratio: float = 0.15,\n",
    "        random_state: int = 0,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        df = pd.read_csv(csv_path)\n",
    "        if random_state is not None:\n",
    "            df = df.sample(frac=1.0, random_state=random_state).reset_index(drop=True)\n",
    "        n = len(df)\n",
    "        val_start, test_start = int(n * (1 - val_ratio - test_ratio)), int(n * (1 - test_ratio))\n",
    "        if split == \"train\":\n",
    "            self.df = df.iloc[:val_start]\n",
    "        elif split == \"val\":\n",
    "            self.df = df.iloc[val_start:test_start]\n",
    "        elif split == \"test\":\n",
    "            self.df = df.iloc[test_start:]\n",
    "        else:\n",
    "            raise ValueError(\"split must be train/val/test\")\n",
    "        self.transform = transform\n",
    "\n",
    "        # Shared, deterministic concept ordering ---------------------------------\n",
    "        if concept_names is None:\n",
    "            # Fall back to keys in the first row of *this* split (legacy)\n",
    "            concept_names = list(json.loads(self.df.iloc[0].concepts).keys())\n",
    "        self.concept_names: List[str] = concept_names\n",
    "        self.loss_mask = torch.tensor(\n",
    "            [0 if k.isupper() else 1 for k in self.concept_names], dtype=torch.float32\n",
    "        )\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(row.image).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        c_dict = json.loads(row.concepts)\n",
    "        vec = [c_dict.get(k, 0.0) for k in self.concept_names]\n",
    "        cvec = torch.tensor(vec, dtype=torch.float32)\n",
    "        label = torch.tensor(row.label, dtype=torch.long)\n",
    "        return img, cvec, label\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    def get_loss_mask(self):\n",
    "        return self.loss_mask\n",
    "\n",
    "    def num_concepts(self):\n",
    "        return len(self.concept_names)\n",
    "\n",
    "# =====================================================================\n",
    "\n",
    "def make_datasets(csv_path: Union[str, Path], transform=None, **kw):\n",
    "    \"\"\"Return (train, val, test) datasets sharing **one** concept ordering.\"\"\"\n",
    "    df_all = pd.read_csv(csv_path)\n",
    "    # union of all keys\n",
    "    all_keys = set()\n",
    "    for js in df_all.concepts:\n",
    "        all_keys.update(json.loads(js).keys())\n",
    "    concept_names = sorted(all_keys)  # deterministic order\n",
    "\n",
    "    return (\n",
    "        AneRBC(csv_path, \"train\", transform, concept_names=concept_names, **kw),\n",
    "        AneRBC(csv_path, \"val\",   transform, concept_names=concept_names, **kw),\n",
    "        AneRBC(csv_path, \"test\",  transform, concept_names=concept_names, **kw),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 5155,
     "status": "ok",
     "timestamp": 1747059057449,
     "user": {
      "displayName": "Long Vu",
      "userId": "17415940645631717193"
     },
     "user_tz": 240
    },
    "id": "UE5HALT0c-An"
   },
   "outputs": [],
   "source": [
    "# === models.py =============================================================\n",
    "\"\"\"Model builders for AneRBC CBM.\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import torchvision.models as tv\n",
    "from torchvision.models import resnet18, ResNet18_Weights, resnet34, ResNet34_Weights\n",
    "\n",
    "\n",
    "class ConceptPredictor(nn.Module):\n",
    "    def __init__(self, num_concepts: int, backbone: str = 'resnet18', pretrained: bool = True,\n",
    "                 freeze_until: Optional[str] = 'layer3'):\n",
    "        super().__init__()\n",
    "        if backbone == 'resnet18':\n",
    "            net = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "            in_feats = net.fc.in_features\n",
    "            net.fc = nn.Identity()\n",
    "        elif backbone == 'resnet34':\n",
    "            net = resnet34(weights=ResNet34_Weights.DEFAULT)\n",
    "            in_feats = net.fc.in_features\n",
    "            net.fc = nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported backbone {backbone}\")\n",
    "        self.backbone = net\n",
    "        if freeze_until:\n",
    "            freeze = True\n",
    "            for name, p in self.backbone.named_parameters():\n",
    "                if name.startswith(freeze_until):\n",
    "                    freeze = False\n",
    "                p.requires_grad = not freeze\n",
    "        self.head = nn.Linear(in_feats, num_concepts)\n",
    "        self.activation = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x)\n",
    "        return self.activation(self.head(feats))\n",
    "\n",
    "class LabelHead(nn.Module):\n",
    "    # def __init__(self, num_concepts: int, hidden: int = 64):\n",
    "    #     super().__init__()\n",
    "    #     self.net = nn.Sequential(\n",
    "    #         nn.Linear(num_concepts, 128),\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.Linear(128, hidden),\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.Linear(hidden, 2)\n",
    "    #     )\n",
    "    def __init__(self, num_concepts: int, hidden=(256, 128, 64), p_drop=0.2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        dims = (num_concepts,) + hidden\n",
    "        for i in range(len(hidden)):\n",
    "            layers += [\n",
    "                nn.Linear(dims[i], dims[i+1]),\n",
    "                nn.BatchNorm1d(dims[i+1]),     # helps convergence\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(p_drop)\n",
    "            ]\n",
    "        layers.append(nn.Linear(hidden[-1], 2))  # logits\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, c):\n",
    "        return self.net(c)\n",
    "\n",
    "class End2EndCBM(nn.Module):\n",
    "    def __init__(self, g: ConceptPredictor, h: LabelHead, loss_mask: torch.Tensor):\n",
    "        super().__init__()\n",
    "        self.g, self.h = g, h\n",
    "        self.register_buffer('loss_mask', loss_mask)\n",
    "    def forward(self, x, c_cbc):\n",
    "        c_pred = self.g(x)\n",
    "        y_logit = self.h(torch.cat([c_pred, c_cbc], dim=1))\n",
    "        return c_pred, y_logit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 840,
     "status": "ok",
     "timestamp": 1747059058291,
     "user": {
      "displayName": "Long Vu",
      "userId": "17415940645631717193"
     },
     "user_tz": 240
    },
    "id": "ILaKf-9hdCyo"
   },
   "outputs": [],
   "source": [
    "# === train.py =============================================================\n",
    "\"\"\"Training script for AneRBC Concept‑Bottleneck (independent / sequential / joint).\"\"\"\n",
    "import argparse, torch, torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pathlib import Path\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, recall_score, precision_score\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser()\n",
    "    p.add_argument('--csv', required=True)\n",
    "    p.add_argument('--batch', type=int, default=32)\n",
    "    p.add_argument('--epochs', type=int, default=20)\n",
    "    p.add_argument('--lr', type=float, default=1e-3)\n",
    "    p.add_argument('--mode', choices=['joint'], default='joint')\n",
    "    return p.parse_args()\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "def make_loaders(csv_path, batch_size):\n",
    "    ''' OLD: Make data loaders without balanced sampling'''\n",
    "    tf = T.Compose([T.Resize((224, 224)), T.ToTensor()])\n",
    "    ds_train, ds_val, ds_test = make_datasets(csv_path, transform=tf)\n",
    "    kw = dict(batch_size=batch_size, num_workers=2, shuffle=True)\n",
    "    print(\"concept_names:\", ds_train.concept_names)  # Order of concepts\n",
    "    return (\n",
    "        DataLoader(ds_train, **kw),\n",
    "        DataLoader(ds_val, batch_size=batch_size, num_workers=2),\n",
    "        DataLoader(ds_test, batch_size=batch_size, num_workers=2),\n",
    "        ds_train.get_loss_mask(),\n",
    "        ds_train.num_concepts(),\n",
    "    )\n",
    "\n",
    "# def make_loaders(csv_path, batch_size):\n",
    "#     ''' Make data loaders with balanced sampling '''\n",
    "#     tf = T.Compose([T.Resize((224, 224)), T.ToTensor()])\n",
    "#     # tf = T.Compose([\n",
    "#     #     T.Resize((224,224)), # Resize before converting to Tensor\n",
    "#     #     T.RandomApply([T.ColorJitter(.2,.2,.2,.05)], p=0.7),\n",
    "#     #     T.RandomRotation(360, interpolation=T.InterpolationMode.BILINEAR),\n",
    "#     #     T.RandomHorizontalFlip(),\n",
    "#     #     T.RandomVerticalFlip(),\n",
    "#     #     T.RandomErasing(p=0.3, scale=(0.01, 0.03)),  # Cutout-ish\n",
    "#     #     T.ToTensor() # Convert to Tensor after applying other image augmentations\n",
    "#     # ])\n",
    "#     ds_train, ds_val, ds_test = make_datasets(csv_path, transform=tf)\n",
    "\n",
    "#     # -------- get labels array for the subset ----------------------------\n",
    "#     if isinstance(ds_train, torch.utils.data.Subset):\n",
    "#         orig = ds_train.dataset                     # AneRBC instance\n",
    "#         idxs = ds_train.indices\n",
    "#         labels = orig.df.iloc[idxs][\"label\"].to_numpy()\n",
    "#     else:\n",
    "#         labels = ds_train.df[\"label\"].to_numpy()\n",
    "\n",
    "#     # -------- build sampler ----------------------------------------------\n",
    "#     class_freq = np.bincount(labels) / len(labels)       # e.g. [0.55, 0.45]\n",
    "#     weights = 1.0 / class_freq[labels]                   # len == len(ds_train)\n",
    "#     sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "\n",
    "#     train_loader = DataLoader(ds_train, batch_size=batch_size,\n",
    "#                               sampler=sampler, num_workers=2)\n",
    "#     val_loader   = DataLoader(ds_val,   batch_size=batch_size, num_workers=2)\n",
    "#     test_loader  = DataLoader(ds_test,  batch_size=batch_size, num_workers=2)\n",
    "\n",
    "#     print(\"concept_names:\", ds_train.dataset.concept_names if isinstance(ds_train, torch.utils.data.Subset) else ds_train.concept_names)\n",
    "#     return train_loader, val_loader, test_loader, \\\n",
    "#            ds_train.dataset.get_loss_mask() if isinstance(ds_train, torch.utils.data.Subset) else ds_train.get_loss_mask(), \\\n",
    "#            ds_train.dataset.num_concepts() if isinstance(ds_train, torch.utils.data.Subset) else ds_train.num_concepts()\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "def train_loop(model, loader, optimiser, loss_c, loss_y, device):\n",
    "    model.train()\n",
    "    tot_c = tot_y = 0\n",
    "    for x, c_all, y in loader:\n",
    "        x, c_all, y = x.to(device), c_all.to(device), y.to(device)\n",
    "        c_cbc = c_all[:, model.loss_mask == 0]\n",
    "        c_gt = c_all[:, model.loss_mask == 1]\n",
    "        optimiser.zero_grad()\n",
    "        c_pred, y_logit = model(x, c_cbc)\n",
    "        # lc = loss_c(c_pred, c_gt).mean()\n",
    "        lc = weighted_concept_loss(c_pred, c_gt)  # weighted concept loss\n",
    "        ly = loss_y(y_logit, y)\n",
    "        (lc + ly).backward()\n",
    "        optimiser.step()\n",
    "        tot_c += lc.item() * len(x)\n",
    "        tot_y += ly.item() * len(x)\n",
    "    return tot_c / len(loader.dataset), tot_y / len(loader.dataset)\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def eval_loop(model, loader, loss_c, loss_y, device):\n",
    "    ''' Model evaluation '''\n",
    "\n",
    "    model.eval()\n",
    "    # Validation loss\n",
    "    tot_c = tot_y = 0\n",
    "    preds, labels = [], []\n",
    "\n",
    "    for x, c_all, y in loader:\n",
    "        x, c_all, y = x.to(device), c_all.to(device), y.to(device)\n",
    "        c_cbc = c_all[:, model.loss_mask == 0]\n",
    "        c_gt = c_all[:, model.loss_mask == 1]\n",
    "        c_pred, y_logit = model(x, c_cbc)\n",
    "\n",
    "        # lc = loss_c(c_pred, c_gt).mean()\n",
    "        lc = weighted_concept_loss(c_pred, c_gt)  # weighted concept loss\n",
    "        ly = loss_y(y_logit, y)\n",
    "        tot_c += lc.item() * len(x)\n",
    "        tot_y += ly.item() * len(x)\n",
    "\n",
    "        # Get predictions and ground truth labels\n",
    "        preds.append(y_logit.argmax(1).cpu())\n",
    "        labels.append(y.cpu())\n",
    "\n",
    "    # Calculate metrics\n",
    "    y_true = torch.cat(labels)\n",
    "    y_pred = torch.cat(preds)\n",
    "    acc    = accuracy_score(y_true, y_pred)\n",
    "    rec    = recall_score(y_true, y_pred)\n",
    "    prec   = precision_score(y_true, y_pred)\n",
    "    f1     = f1_score(y_true, y_pred)\n",
    "\n",
    "    acc = accuracy_score(torch.cat(labels), torch.cat(preds))\n",
    "    return tot_c / len(loader.dataset), tot_y / len(loader.dataset), acc, rec, prec, f1\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "def weighted_concept_loss(pred, target):\n",
    "    \"\"\"\n",
    "    pred, target: tensors [B, K_total]\n",
    "    Only the 12 morph positions (mask==1) are weighted; CBC columns remain masked out.\n",
    "    \"\"\"\n",
    "    # element‑wise BCE\n",
    "    raw = bce(pred, target)                          # B × K_total\n",
    "    # broadcast pos_weight across batch, using mask to select morph cols\n",
    "    w   = torch.zeros_like(raw, device=raw.device)\n",
    "    relevant_pos_weight = pos_weight[mask == 1]  # Length 15, for 15 morphology flags\n",
    "    w[:, morph_mask.bool()] = relevant_pos_weight.type(torch.float32) # CBC cols stay 0 (will be masked)\n",
    "    return (morph_mask * w * raw).mean()                  # scalar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 498
    },
    "executionInfo": {
     "elapsed": 141161,
     "status": "error",
     "timestamp": 1747059199516,
     "user": {
      "displayName": "Long Vu",
      "userId": "17415940645631717193"
     },
     "user_tz": 240
    },
    "id": "aXKnZjZCevwq",
    "outputId": "5d8eef00-ee34-480c-8bf8-691e00777db3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concept_names: ['HCT', 'HGB', 'MCH', 'MCV', 'RBC', 'anisocytosis', 'elliptocytes', 'hypochromic', 'microcytic', 'monocytosis', 'neutrophilia', 'normochromic', 'normocytic', 'platelets_decreased', 'platelets_increased', 'polychromasia', 'reactive_lymphocytes', 'target_cells', 'tear_drop_cells', 'thalassemia']\n",
      "mask=  tensor([0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n",
      "100%|██████████| 83.3M/83.3M [00:00<00:00, 218MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run #1 ========================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-65e3efcc7ba2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0mbest_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m       \u001b[0mlc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m       \u001b[0mvlc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvacc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvprec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvaL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"E{epoch:02d} trainC {lc:.3f} valC {vlc:.3f}  trainY {ly:.3f} valY {vly:.3f}  acc {vacc:.3f} recall {vrecall:.3f} prec {vprec:.3f} f1 {vf1:.3f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-29ac20932731>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(model, loader, optimiser, loss_c, loss_y, device)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mtot_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtot_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mc_cbc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_mask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1459\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1419\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1421\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import types\n",
    "\n",
    "args = types.SimpleNamespace()\n",
    "args.csv = '/content/drive/MyDrive/Trustworthy_AI_Final_Project/CBM_w_AneRBC/anerbc_manifest_filtered_out_bad_data.csv'  # <--- Set your required CSV path here\n",
    "args.batch = 32\n",
    "args.epochs = 30\n",
    "args.lr = 1e-3\n",
    "args.mode = 'joint' # Although choices were defined, here you just set the value\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "bce = nn.BCELoss(reduction='none')\n",
    "prevalence = torch.tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.300, 0.418, 0.526, 0.486, 0.055, 0.070, 0.471, 0.500,\n",
    "        0.072, 0.140, 0.234, 0.057, 0.205, 0.067, 0.072],\n",
    "       device='cuda:0', dtype=torch.float64)\n",
    "\n",
    "pos_weight = (1.0 - prevalence) / prevalence          # inverse odds, shape (12,)\n",
    "\n",
    "trL, vaL, teL, mask, num_c = make_loaders(args.csv, args.batch)\n",
    "print(\"mask= \", mask)\n",
    "n_morph = int(mask.sum().item())  # count of supervised morphology flags\n",
    "g = ConceptPredictor(n_morph, backbone=\"resnet34\")  # predicts only morph flags\n",
    "h = LabelHead(num_c)\n",
    "morph_mask = mask[mask == 1].to(device)\n",
    "model = End2EndCBM(g, h, mask).to(device)\n",
    "# opt = torch.optim.AdamW([{'params':g.backbone.parameters(), 'lr':1e-4}, {'params':g.head.parameters()}, {'params':h.parameters()}], lr=args.lr)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=args.lr)\n",
    "loss_c = nn.BCELoss(reduction='none')\n",
    "loss_y = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.3, patience=3)\n",
    "\n",
    "test_acc = 0.\n",
    "run = 1\n",
    "while test_acc < 0.1:\n",
    "  print(f\"Run #{run} ========================================\")\n",
    "  best_val_acc = 0.\n",
    "  best_f1 = 0.\n",
    "  for epoch in range(1, args.epochs+1):\n",
    "      lc, ly = train_loop(model, trL, opt, loss_c, loss_y, device)\n",
    "      vlc, vly, vacc, vrecall, vprec, vf1 = eval_loop(model, vaL, loss_c, loss_y, device)\n",
    "      print(f\"E{epoch:02d} trainC {lc:.3f} valC {vlc:.3f}  trainY {ly:.3f} valY {vly:.3f}  acc {vacc:.3f} recall {vrecall:.3f} prec {vprec:.3f} f1 {vf1:.3f}\")\n",
    "\n",
    "      if vacc>best_val_acc:\n",
    "          best_val_acc = vacc\n",
    "          torch.save(model.state_dict(), 'best.ckpt')\n",
    "      elif vf1>best_f1:\n",
    "          best_f1 = vf1\n",
    "          torch.save(model.state_dict(), 'best.ckpt')\n",
    "\n",
    "      scheduler.step(vly)\n",
    "\n",
    "  # final test\n",
    "  model.load_state_dict(torch.load('best.ckpt'))\n",
    "  _, _, test_acc, test_recall, test_prec, test_f1 = eval_loop(model, teL, loss_c, loss_y, device)\n",
    "  torch.save(model.state_dict(), f'/content/drive/MyDrive/Trustworthy_AI_Final_Project/CBM_w_AneRBC/checkpoints/acc_0{int(test_acc * 1000)}_f1_0{int(test_f1 * 1000)}.ckpt')\n",
    "  print(f\"TEST accuracy: {test_acc:.3f}\")\n",
    "  print(f\"TEST f1: {test_f1:.3f}\")\n",
    "  run += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WAtoO1cjCOG"
   },
   "source": [
    "# Load-and-Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7608,
     "status": "ok",
     "timestamp": 1747059910335,
     "user": {
      "displayName": "Long Vu",
      "userId": "17415940645631717193"
     },
     "user_tz": 240
    },
    "id": "W6H3rXhRjBvZ",
    "outputId": "37d0c826-3527-4bd9-a9b6-36f205674ca4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anemia probability: 0.979\n",
      "anisocytosis        0.36\n",
      "elliptocytes        0.98\n",
      "hypochromic         1.00\n",
      "microcytic          1.00\n",
      "monocytosis         0.00\n",
      "neutrophilia        0.03\n",
      "normochromic        0.01\n",
      "normocytic          0.00\n",
      "platelets_decreased 0.00\n",
      "platelets_increased 0.55\n",
      "polychromasia       0.38\n",
      "reactive_lymphocytes0.00\n",
      "target_cells        0.78\n",
      "tear_drop_cells     0.00\n",
      "thalassemia         0.02\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "#   helper functions\n",
    "# ------------------------------------------------------------\n",
    "import re\n",
    "\n",
    "CBC_CANON = {\n",
    "    # canonical : list of regex patterns covering spelling variants\n",
    "    \"HGB\": [r\"\\bHGB\\b\", r\"\\bH\\.?emoglobin\\b\", r\"\\bHemoglobin\\b\"],\n",
    "    \"HCT\": [r\"\\bHCT\\b\", r\"\\bH\\.?ematocrit\\b\", r\"\\bHematocrit\\b\"],\n",
    "    \"RBC\": [r\"\\bRBC\\b\", r\"\\bRed\\s+Blood\\s+Cells?\\b\"],\n",
    "    \"MCV\": [r\"\\bMCV\\b\", r\"\\bMean\\s+Corpuscular\\s+Volume\\b\"],\n",
    "    \"MCH\": [r\"\\bMCH\\b\", r\"\\bMean\\s+Corpuscular\\s+Hemoglobin\\b\"],\n",
    "}\n",
    "\n",
    "CBC_RE = {k: re.compile('|'.join(pat), re.I) for k, pat in CBC_CANON.items()}\n",
    "VALUE_RE = re.compile(r\"([0-9]*\\.?[0-9]+)\")\n",
    "\n",
    "def parse_cbc(text: str) -> dict:\n",
    "    out = {k: None for k in CBC_CANON}\n",
    "    for line in text.splitlines():\n",
    "        for key, rx in CBC_RE.items():\n",
    "            if rx.search(line):\n",
    "                m = VALUE_RE.search(line)\n",
    "                if m:\n",
    "                    out[key] = float(m.group(1))\n",
    "    # fallback: set missing values to 0.0 or NaN\n",
    "    return {k: (v if v is not None else 0.0) for k, v in out.items()}\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "#   Load-and-Predict\n",
    "# ------------------------------------------------------------\n",
    "from pathlib import Path\n",
    "\n",
    "# 0. imports & helpers\n",
    "import json, torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# 1. recreate the concept list & loss-mask (same code as training)\n",
    "csv_path = \"/content/drive/MyDrive/Trustworthy_AI_Final_Project/CBM_w_AneRBC/anerbc_manifest_filtered_out_bad_data.csv\"\n",
    "_, _, ds_test = make_datasets(csv_path)      # we only need one split\n",
    "concept_names = ds_test.concept_names\n",
    "loss_mask     = ds_test.get_loss_mask()\n",
    "n_morph       = int(loss_mask.sum().item())  # 14\n",
    "\n",
    "# 2. rebuild the architecture\n",
    "g = ConceptPredictor(n_morph, backbone=\"resnet34\", pretrained=False)   # weights will be overwritten\n",
    "h = LabelHead(len(concept_names))\n",
    "model = End2EndCBM(g, h, loss_mask)\n",
    "model.load_state_dict(torch.load(\"/content/drive/MyDrive/Trustworthy_AI_Final_Project/CBM_w_AneRBC/checkpoints/resnet32_mlp_256_128_64/acc_0909_f1_0917.ckpt\", map_location=\"cpu\"))\n",
    "model.load_state_dict(torch.load(\"/content/drive/MyDrive/Trustworthy_AI_Final_Project/CBM_w_AneRBC/checkpoints/resnet32_mlp_256_128_64/acc_0916_f1_0919_wbce.ckpt\", map_location=\"cpu\"))\n",
    "model.eval()\n",
    "\n",
    "# 3. image transform (same as train)\n",
    "transform = T.Compose([T.Resize((224,224)), T.ToTensor()])\n",
    "\n",
    "# 4. helper to run one smear + CBC vector\n",
    "def predict(image_path: str, cbc_dict: dict[str, float]):\n",
    "    \"\"\"Return (p_anemic, concept_dict_pred). `cbc_dict` keys must match the manifest order.\"\"\"\n",
    "    # --- prepare tensors -------------------------------------------------\n",
    "    img  = transform(Image.open(image_path).convert(\"RGB\")).unsqueeze(0)  # 1×3×224×224\n",
    "    c_cbc = torch.tensor([[cbc_dict.get(k, 0.0) for k in concept_names if k.isupper()]],\n",
    "                         dtype=torch.float32)                             # 1×9\n",
    "\n",
    "    # --- forward ---------------------------------------------------------\n",
    "    with torch.inference_mode():\n",
    "        c_pred, y_logit = model(img, c_cbc)\n",
    "        p_anemic = torch.softmax(y_logit, dim=1)[0,1].item()\n",
    "\n",
    "        # unpack the 14 predicted morphology bits\n",
    "        morph_keys = [k for k in concept_names if not k.isupper()]\n",
    "        c_vec = c_pred.squeeze().tolist()\n",
    "        concept_pred = dict(zip(morph_keys, c_vec))\n",
    "\n",
    "    return p_anemic, concept_pred\n",
    "\n",
    "# 5. example call ---------------------------------------------------------\n",
    "ROOT = Path(\"/content/drive/MyDrive/Trustworthy_AI_Final_Project/CBM_w_AneRBC/AneRBC_full/AneRBC/AneRBC_dataset/AneRBC-I/\")\n",
    "anemia_labels = [\"Healthy_individuals\", \"Anemic_individuals\"]\n",
    "label = anemia_labels[1]\n",
    "stem = \"054_a\"  # _a for anemia, _h for healthy\n",
    "\n",
    "img_path = ROOT / f\"{label}/Original_images\" / f\"{stem}.png\"\n",
    "cbc_path = ROOT / f\"{label}/CBC_reports\" / f\"{stem}.txt\"\n",
    "\n",
    "cbc_sample = parse_cbc(Path(cbc_path).read_text())\n",
    "p, concepts = predict(img_path, cbc_sample)\n",
    "\n",
    "print(f\"Anemia probability: {p:.3f}\")\n",
    "for k,v in concepts.items():\n",
    "    print(f\"{k:20s}{v:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00z7fT9LRR-Y"
   },
   "source": [
    "# AUROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8499,
     "status": "ok",
     "timestamp": 1747005081007,
     "user": {
      "displayName": "Long Vu",
      "userId": "17415940645631717193"
     },
     "user_tz": 240
    },
    "id": "qXSEMajIblvf",
    "outputId": "89c634da-d3fb-4669-b4d1-b94e8045cf74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Concept AUROC:\n",
      "microcytic                0.895\n",
      "normocytic                0.146\n",
      "normochromic              0.121\n",
      "hypochromic               0.944\n",
      "elliptocytes              0.729\n",
      "target_cells              0.488\n",
      "tear_drop_cells           0.224\n",
      "anisocytosis              0.223\n",
      "polychromasia             0.536\n",
      "neutrophilia              0.605\n",
      "monocytosis               0.706\n",
      "reactive_lymphocytes      0.920\n",
      "platelets_increased       0.644\n",
      "platelets_decreased       0.670\n",
      "thalassemia               0.900\n",
      "Macro‑average AUROC (ignoring NaNs): 0.583\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "#  AUROC for the supervised morphology flags\n",
    "# ------------------------------------------------------------\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np, torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def concept_auroc(model, loader, morph_mask, device='cpu'):\n",
    "    \"\"\"\n",
    "    Returns list of AUROCs (len = # supervised morph flags)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    for x, c_all, _ in loader:\n",
    "        x, c_all = x.to(device), c_all.to(device)\n",
    "        c_gt   = c_all[:, morph_mask.bool()]              # B × K_morph\n",
    "        c_cbc  = c_all[:, ~morph_mask.bool()]             # B × 5 CBC\n",
    "        c_pred, _ = model(x, c_cbc)\n",
    "\n",
    "        y_true.append(c_gt.cpu())\n",
    "        y_pred.append(c_pred.cpu())\n",
    "\n",
    "    y_true = torch.cat(y_true).numpy()\n",
    "    y_pred = torch.cat(y_pred).numpy()\n",
    "\n",
    "    aurocs = []\n",
    "    for k in range(y_true.shape[1]):\n",
    "        # skip if concept is all-0 or all-1 in this split\n",
    "        if y_true[:, k].sum() in (0, len(y_true)):\n",
    "            aurocs.append(np.nan)\n",
    "        else:\n",
    "            aurocs.append(roc_auc_score(y_true[:, k], y_pred[:, k]))\n",
    "    return aurocs\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0. helpers already defined: ConceptPredictor, LabelHead, End2EndCBM, concept_auroc\n",
    "# ------------------------------------------------------------\n",
    "csv_path = \"/content/drive/MyDrive/Trustworthy_AI_Final_Project/CBM_w_AneRBC/anerbc_manifest_filtered_out_bad_data.csv\"\n",
    "\n",
    "# transform identical to training\n",
    "import torchvision.transforms as T\n",
    "tf = T.Compose([T.Resize((224,224)), T.ToTensor()])\n",
    "\n",
    "# 1. rebuild test dataset WITH transform\n",
    "ds_test = AneRBC(csv_path, split=\"test\", transform=tf)   # <- single creation\n",
    "concept_names = ds_test.concept_names\n",
    "mask          = ds_test.get_loss_mask()\n",
    "morph_mask    = mask == 1                                # boolean of length 22\n",
    "\n",
    "# 2. DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "teL = DataLoader(ds_test, batch_size=64, shuffle=False)\n",
    "\n",
    "# 3. rebuild model skeleton\n",
    "n_morph = int(morph_mask.sum().item())                   # 12 supervised flags\n",
    "g = ConceptPredictor(n_morph, backbone=\"resnet34\", pretrained=False)\n",
    "h = LabelHead(len(concept_names), hidden=(256,128,64), p_drop=0.2)\n",
    "model = End2EndCBM(g, h, mask)\n",
    "state = torch.load(\"/content/drive/MyDrive/Trustworthy_AI_Final_Project/CBM_w_AneRBC/checkpoints/resnet32_mlp_256_128_64/acc_0909_f1_0917.ckpt\",\n",
    "                   map_location=\"cpu\")\n",
    "model.load_state_dict(state, strict=True)\n",
    "model.eval()\n",
    "\n",
    "# 4. AUROC\n",
    "aurocs = concept_auroc(model, teL, morph_mask, device='cpu')\n",
    "morph_keys = [k for k in concept_names if not k.isupper()]\n",
    "\n",
    "print(\"\\nConcept AUROC:\")\n",
    "for k, auc in zip(morph_keys, aurocs):\n",
    "    print(f\"{k:25s} {auc:.3f}\")\n",
    "print(f\"Macro‑average AUROC (ignoring NaNs): {np.nanmean(aurocs):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jwa6DtPNVVdx"
   },
   "source": [
    "# Prevalence for Weighted BCE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 134,
     "status": "ok",
     "timestamp": 1747059242990,
     "user": {
      "displayName": "Long Vu",
      "userId": "17415940645631717193"
     },
     "user_tz": 240
    },
    "id": "3zMzVB0VVVIb",
    "outputId": "473353a5-e6fc-49bc-fcd9-0e99e4ec70b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HCT                  0.000\n",
      "HGB                  0.000\n",
      "MCH                  0.000\n",
      "MCV                  0.000\n",
      "RBC                  0.000\n",
      "anisocytosis         0.300\n",
      "elliptocytes         0.418\n",
      "hypochromic          0.526\n",
      "microcytic           0.486\n",
      "monocytosis          0.055\n",
      "neutrophilia         0.070\n",
      "normochromic         0.471\n",
      "normocytic           0.500\n",
      "platelets_decreased  0.072\n",
      "platelets_increased  0.140\n",
      "polychromasia        0.234\n",
      "reactive_lymphocytes 0.057\n",
      "target_cells         0.205\n",
      "tear_drop_cells      0.067\n",
      "thalassemia          0.072\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "\n",
    "# Load your manifest CSV\n",
    "csv_path = \"/content/drive/MyDrive/Trustworthy_AI_Final_Project/CBM_w_AneRBC/anerbc_manifest_filtered_out_bad_data.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Get all concept keys\n",
    "all_keys = set()\n",
    "for js in df.concepts:\n",
    "    all_keys.update(json.loads(js).keys())\n",
    "\n",
    "# Sort concept keys to ensure consistent order\n",
    "concept_names = sorted(list(all_keys))\n",
    "\n",
    "# Calculate prevalence for each concept (except HGB)\n",
    "prevalence_dict = {}\n",
    "for concept in concept_names:\n",
    "    if concept != \"HGB\":  # Exclude HGB\n",
    "        total_count = len(df)\n",
    "        positive_count = df['concepts'].apply(lambda x: json.loads(x).get(concept, 0) == 1).sum()\n",
    "        prevalence_dict[concept] = positive_count / total_count\n",
    "\n",
    "# Create prevalence tensor with HGB prevalence as 0\n",
    "prevalence_list = [0.0]  # Start with HGB prevalence as 0\n",
    "for concept in concept_names:\n",
    "    if concept != \"HGB\":\n",
    "        prevalence_list.append(prevalence_dict[concept])\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "prevalence = torch.tensor(prevalence_list, device=device)\n",
    "\n",
    "# Print concept names and their prevalence values\n",
    "for i, concept in enumerate(concept_names):\n",
    "    print(f\"{concept:20s} {prevalence[i].item():.3f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOGSaDjQ9bbY/BX/KtPzzlA",
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
